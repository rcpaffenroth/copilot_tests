{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f0dab1",
   "metadata": {},
   "source": [
    "# Install JAX\n",
    "\n",
    "To run this notebook, JAX must be installed. If running locally, JAX can be installed for CPU with:\n",
    "\n",
    "```bash\n",
    "pip install jax jaxlib\n",
    "```\n",
    "\n",
    "For GPU support, see the [JAX installation guide](https://github.com/google/jax#installation) for the correct pip command for your CUDA version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005b947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09cb12",
   "metadata": {},
   "source": [
    "# Define the Least Squares Problem\n",
    "\n",
    "We want to solve the linear system $Ax = b$ in the least squares sense, i.e., find $x$ that minimizes $\\|Ax - b\\|^2$. We'll generate a random matrix $A$ and vector $b$ for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce15a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-05-20 08:37:18,395:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =\n",
      " [[ 1.6226422   2.0252647 ]\n",
      " [-0.43359444 -0.07861735]\n",
      " [ 0.1760909  -0.97208923]\n",
      " [-0.49529874  0.4943786 ]\n",
      " [ 0.6643493  -0.9501635 ]]\n",
      "b = [-2.6682458  -0.42881033  3.22509    -2.481595    4.1967983 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "A = random.normal(key, (5, 2))  # 5 equations, 2 unknowns\n",
    "x_true = jnp.array([2.0, -3.0])\n",
    "b = A @ x_true + 0.1 * random.normal(key, (5,))  # Add a bit of noise\n",
    "print(\"A =\\n\", A)\n",
    "print(\"b =\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c31d57",
   "metadata": {},
   "source": [
    "# Solve the Least Squares Problem Using JAX\n",
    "\n",
    "We'll use JAX's linear algebra routines to solve for $x$ that minimizes $\\|Ax - b\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3e9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated x: [ 2.022512  -2.9543445]\n",
      "True x: [ 2. -3.]\n",
      "Residual norm: 0.22407445\n"
     ]
    }
   ],
   "source": [
    "# Compute the least squares solution using the pseudo-inverse\n",
    "x_lstsq = jnp.linalg.pinv(A) @ b\n",
    "print(\"Estimated x:\", x_lstsq)\n",
    "print(\"True x:\", x_true)\n",
    "print(\"Residual norm:\", jnp.linalg.norm(A @ x_lstsq - b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8770d32",
   "metadata": {},
   "source": [
    "# Solving Least Squares with Gradient Descent\n",
    "\n",
    "Instead of using the pseudo-inverse, we can solve the least squares problem by minimizing the loss $L(x) = \\|Ax - b\\|^2$ using gradient descent. JAX makes it easy to compute gradients and perform optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed36805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 15.1927, x: [ 0.04415018 -1.3719759 ]\n",
      "Step 20, loss: 0.0508, x: [ 2.0087342 -2.9464455]\n",
      "Step 40, loss: 0.0502, x: [ 2.0224242 -2.9542947]\n",
      "Step 60, loss: 0.0502, x: [ 2.022511  -2.9543443]\n",
      "Step 80, loss: 0.0502, x: [ 2.022511  -2.9543443]\n",
      "\n",
      "Gradient Descent Solution: [ 2.022511  -2.9543443]\n",
      "True x: [ 2. -3.]\n",
      "Residual norm: 0.22407433\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import grad\n",
    "\n",
    "# Define the loss function\n",
    "loss = lambda x: jnp.sum((A @ x - b) ** 2)\n",
    "\n",
    "# Compute the gradient of the loss\n",
    "loss_grad = grad(loss)\n",
    "\n",
    "# Gradient descent loop\n",
    "x_gd = jnp.zeros_like(x_true)\n",
    "learning_rate = 0.05\n",
    "num_steps = 100\n",
    "\n",
    "for i in range(num_steps):\n",
    "    x_gd -= learning_rate * loss_grad(x_gd)\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Step {i}, loss: {loss(x_gd):.4f}, x: {x_gd}\")\n",
    "\n",
    "print(\"\\nGradient Descent Solution:\", x_gd)\n",
    "print(\"True x:\", x_true)\n",
    "print(\"Residual norm:\", jnp.linalg.norm(A @ x_gd - b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50ca2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
